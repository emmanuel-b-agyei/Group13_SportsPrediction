# -*- coding: utf-8 -*-
"""Group13_SportsPrediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1T63lEqE-rhRazDq3wtv623UrFyB8yG4m
"""

import numpy as np, pandas as pd
import matplotlib.pyplot as plt
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from xgboost import XGBRegressor
from sklearn.metrics import accuracy_score
from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn import tree, metrics
from google.colab import drive
import pickle
import joblib
drive.mount('/content/drive')

df=pd.read_csv('/content/drive/My Drive/Colab Notebooks/players_21.csv')

"""USING CORRELATION MATRIX TO DETERMINE WHAT COLUMNS TO DROP"""

correlation_matrix = df.corr()
correlation_with_target = correlation_matrix['overall']

# Selecting features with high correlation in order to get good results
selected_features = correlation_with_target[abs(correlation_with_target) > 0.5].index
print(selected_features)

"""REMOVING IRRELEVANT VARIABLES BASED ON CORRELATION MATRIX AND WHAT WE DECIDED WAS IRRELEVANT


"""

df = df.drop(['ls','st','rs','lw','lf','cf','rf','rw','lam','cam','ram','lm','lcm','cm','rcm','rm','lwb', 'ldm','cdm','rdm','rwb','lb','lcb','cb','rcb','rb','gk','player_traits','player_tags','body_type','work_rate','preferred_foot','nation_position','nationality_name','club_position','club_name','league_name','height_cm','weight_kg','league_level','weak_foot','skill_moves','international_reputation','pace','shooting','defending','attacking_crossing','attacking_finishing','attacking_heading_accuracy','attacking_volleys','skill_dribbling','skill_curve','skill_fk_accuracy','skill_long_passing','skill_ball_control',   'short_name','long_name', 'release_clause_eur',  'sofifa_id', 'player_url', 'player_face_url', 'real_face', 'club_logo_url','club_flag_url','nation_logo_url','nation_flag_url','dob','club_contract_valid_until',
                   'club_joined','club_jersey_number','nationality_id','club_loaned_from','nation_jersey_number','real_face','club_team_id','nation_team_id','movement_acceleration','movement_sprint_speed','movement_agility','movement_balance','power_jumping','power_stamina','power_strength','power_long_shots',
              'mentality_aggression','mentality_positioning','mentality_penalties','mentality_interceptions','defending_marking_awareness','defending_standing_tackle','defending_sliding_tackle','goalkeeping_diving','goalkeeping_handling','goalkeeping_kicking','goalkeeping_positioning','goalkeeping_reflexes','goalkeeping_speed',


              ],axis=1)

df.info()

"""VISUAL CORRELATION MATRIX- to provide visual representation of a data-based justification of  why certain columns will be dropped




"""

corr_matrix = df.corr()
plt.figure(figsize=(12, 8))
plt.imshow(corr_matrix, cmap='coolwarm', interpolation='nearest')
plt.colorbar()
plt.title('Correlation Matrix')
plt.xticks(range(len(corr_matrix.columns)), corr_matrix.columns, rotation=90)
plt.yticks(range(len(corr_matrix.columns)), corr_matrix.columns)
plt.show()

"""CLEANING DATA: Handling Missing values"""

# separating numeric and non-numeric columns
numeric_columns = df.select_dtypes(include = [np.number]).columns
non_numeric_columns = df.select_dtypes(exclude = [np.number]).columns

numeric_imputer = SimpleImputer(strategy = 'median')
non_numeric_imputer = SimpleImputer(strategy = 'most_frequent')

df[numeric_columns] = numeric_imputer.fit_transform(df[numeric_columns])
df[non_numeric_columns] = non_numeric_imputer.fit_transform(df[non_numeric_columns])

df

"""ENCODING CATEGORICAL VARIABLES"""

# Encoding categorical variables using OneHotEncoder

column_to_encode = ['player_positions']

# Applying one-hot encoding to the selected columns
df_encoded = pd.get_dummies(df, columns=column_to_encode)

# Displaying the encoded DataFrame
pd.set_option('display.max_columns', 1764)
df_encoded

"""SELECTING DEPENDENT AND INDEPENDENT VARIABLES"""

y = df_encoded['overall']
x = df_encoded.drop(['overall'], axis =1)

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

"""SCALING THE DATA"""

scaler = StandardScaler()
numeric_features = x.select_dtypes(include = ['int', 'float']).columns
x_train_scaled = scaler.fit_transform(x_train[numeric_features])
x_test_scaled = scaler.transform(x_test[numeric_features])

print(x_train_scaled)
print(x_test_scaled)

"""TRAINING WITH CROSS VALIDATION"""

# RANDOM FOREST

rf = RandomForestRegressor()
rf.fit(x_train_scaled, y_train)

# Cross-validation for Random Forest
rf_scores = cross_val_score(rf, x_train_scaled, y_train, cv=5, scoring='neg_mean_squared_error')
rf_rmse_scores = np.sqrt(-rf_scores)

# Optimizing Random Forest model performan
param_grid = [
    {'n_estimators': [90,100,200,300,350], 'max_features': [60,80,100,120,150]}
    #{'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},
  ]

grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='neg_mean_squared_error')
grid_search.fit(x_train_scaled,y_train)
rf_best_params = grid_search.best_params_


# Random Forest Evaluation
rf_pred = rf.predict(x_test_scaled)
rf_mse = mean_squared_error(y_test, rf_pred)
rf_rmse = np.sqrt(rf_mse)


print("Random Forest RMSE during Cross-Validation:", rf_rmse_scores)
print("Best Parameters for Random Forest:", rf_best_params)
print("Random Forest RMSE on Validation Set:", rf_rmse)

# XGBOOST

xg_reg = XGBRegressor(objective ='reg:squarederror')
xg_reg.fit(x_train_scaled , y_train)

# Cross-validation for XGBoost
xgb_scores = cross_val_score(xg_reg, x_train_scaled, y_train, cv=5, scoring='neg_mean_squared_error')
xgb_rmse_scores = np.sqrt(-xgb_scores)

# selecting best parameters to optimize XGBoost performance
params = {"learning_rate": [0.01,0.1,0.2,0.5], "max_depth": [8,25,50,100], "min_child_weight": [5,10,15]}
xgb_grid = GridSearchCV(xg_reg, params, cv=5, scoring='neg_mean_squared_error')
xgb_grid.fit(x_train_scaled, y_train)
xgb_best_params = xgb_grid.best_params_

# XGBoost evaluation
xgb_pred = xg_reg.predict(x_test_scaled)
xgb_mse = mean_squared_error(y_test, xgb_pred)
xgb_rmse = np.sqrt(xgb_mse)


print("\nXGBoost RMSE during Cross-Validation:", xgb_rmse_scores)
print("Best Parameters for XGBoost:", xgb_best_params)
print("XGBoost RMSE on Validation Set:", xgb_rmse)

# GRADIENT BOOSTING REGRESSOR

gbr = GradientBoostingRegressor()
gbr.fit(x_train_scaled, y_train)

# Cross-validation for Gradient Boosting Regressor
gbr_scores = cross_val_score(gbr, x_train_scaled, y_train, cv=5, scoring='neg_mean_squared_error')
gbr_rmse_scores = np.sqrt(-gbr_scores)

# Hyperparameter tuning for Gradient Boosting Regressor
param_grid_gbr = {'n_estimators': [1300, 1500,2000], 'learning_rate': [0.18, 0.2, 0.22]}
gbr_grid = GridSearchCV(gbr, param_grid_gbr, cv=5, scoring='neg_mean_squared_error')
gbr_grid.fit(x_train_scaled, y_train)
gbr_best_params = gbr_grid.best_params_

# Gradient Boosting evaluation
gbr_pred = gbr.predict(x_test_scaled)
gbr_mse = mean_squared_error(y_test, gbr_pred)
gbr_rmse = np.sqrt(gbr_mse)

print("\nGradient Boosting Regressor RMSE during Cross-Validation:", gbr_rmse_scores)
print("Best Parameters for Gradient Boosting Regressor:", gbr_best_params)
print("Gradient Boosting Regressor RMSE on Validation Set:", gbr_rmse)

"""DISPLAYING MODEL PERFORMANCE AND EVALUATION"""

print("Random Forest Validation Score:", rf_pred)
print("XGBoost Validation Score:", xgb_pred)
print("Gradient Boosting Validation Score:", gbr_pred)
print()

print("Random Forest root mean square error:", rf_rmse)
print("XGBoost root mean square error:", xgb_rmse)
print("Gradient Boosting root mean square error:", gbr_rmse)

"""TESTING THE MODEL USING THE 2022 DATASET"""

# LOADING AND CLEANING THE DATA

df=pd.read_csv('/content/drive/My Drive/Colab Notebooks/players_22.csv')

df = df.drop(['ls','st','rs','lw','lf','cf','rf','rw','lam','cam','ram','lm','lcm','cm','rcm','rm','lwb', 'ldm','cdm','rdm','rwb','lb','lcb','cb','rcb','rb','gk','player_traits','player_tags','body_type','work_rate','preferred_foot','nation_position','nationality_name','club_position','club_name','league_name','height_cm','weight_kg','league_level','weak_foot','skill_moves','international_reputation','pace','shooting','defending','attacking_crossing','attacking_finishing','attacking_heading_accuracy','attacking_volleys','skill_dribbling','skill_curve','skill_fk_accuracy','skill_long_passing','skill_ball_control',   'short_name','long_name', 'release_clause_eur',  'sofifa_id', 'player_url', 'player_face_url', 'real_face', 'club_logo_url','club_flag_url','nation_logo_url','nation_flag_url','dob','club_contract_valid_until',
                   'club_joined','club_jersey_number','nationality_id','club_loaned_from','nation_jersey_number','real_face','club_team_id','nation_team_id','movement_acceleration','movement_sprint_speed','movement_agility','movement_balance','power_jumping','power_stamina','power_strength','power_long_shots',
              'mentality_aggression','mentality_positioning','mentality_penalties','mentality_interceptions','defending_marking_awareness','defending_standing_tackle','defending_sliding_tackle','goalkeeping_diving','goalkeeping_handling','goalkeeping_kicking','goalkeeping_positioning','goalkeeping_reflexes','goalkeeping_speed',


              ],axis=1)

numeric_columns = df.select_dtypes(include = [np.number]).columns
non_numeric_columns = df.select_dtypes(exclude = [np.number]).columns

numeric_imputer = SimpleImputer(strategy = 'median')
non_numeric_imputer = SimpleImputer(strategy = 'most_frequent')

df[numeric_columns] = numeric_imputer.fit_transform(df[numeric_columns])
df[non_numeric_columns] = non_numeric_imputer.fit_transform(df[non_numeric_columns])

column_to_encode = ['player_positions']

# Applying one-hot encoding to the selected columns
df_encoded = pd.get_dummies(df, columns=column_to_encode)

y = df_encoded['overall']
x = df_encoded.drop(['overall'], axis =1)

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
numeric_features = x.select_dtypes(include = ['int', 'float']).columns
x_player22_train_scaled = scaler.fit_transform(x_train[numeric_features])
x_player22_test_scaled = scaler.transform(x_test[numeric_features])

# TESTING THE GRADIENT BOOSTING REGRESSOR MODEL
gbr_pred = gbr.predict(x_player22_test_scaled)
gbr_mse = mean_squared_error(y_test, gbr_pred)
gbr_rmse = np.sqrt(gbr_mse)

print("Gradient Boosting Regressor RMSE on Validation Set:", gbr_rmse)

"""MAKING CONNECTION WITH API"""

import pickle

pickle.dump(gbr, open('/content/drive/My Drive/Colab Notebooks/Group13_SportsPrediction.pkl', 'wb'))

model = pickle.load(open('/content/drive/My Drive/Colab Notebooks/Group13_SportsPrediction.pkl', 'rb'))