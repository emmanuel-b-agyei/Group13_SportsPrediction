# -*- coding: utf-8 -*-
"""Group13_SportsPrediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jhzfQdlG662dA3Cmg70m0ECnnmYdqxMd
"""

import numpy as np, pandas as pd
import matplotlib.pyplot as plt
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from google.colab import drive
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from xgboost import XGBRegressor
from sklearn.metrics import accuracy_score
from sklearn.metrics import mean_absolute_error, mean_squared_error

drive.mount('/content/drive')

df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/players_21.csv')

"""REMOVING IRRELEVANT VARIABLES"""

df = df.drop(
    ['sofifa_id', 'player_url', 'player_face_url', 'real_face', 'club_logo_url', 'club_flag_url', 'nation_logo_url',
     'nation_flag_url', 'dob', 'club_contract_valid_until',
     'club_joined', 'club_jersey_number', 'nationality_id', 'club_loaned_from', 'nation_jersey_number', 'real_face',
     'club_team_id', 'nation_team_id',
     ], axis=1)

"""CLEANING DATA: Handling Missing values"""

# separating numeric and non-numeric columns
numeric_columns = df.select_dtypes(include=[np.number]).columns
non_numeric_columns = df.select_dtypes(exclude=[np.number]).columns

numeric_imputer = SimpleImputer(strategy='mean')
non_numeric_imputer = SimpleImputer(strategy='most_frequent')

df[numeric_columns] = numeric_imputer.fit_transform(df[numeric_columns])
df[non_numeric_columns] = non_numeric_imputer.fit_transform(df[non_numeric_columns])

df

"""ENCODING CATEGORICAL VARIABLES"""

# Encoding categorical variables using OneHotEncoder

columns_to_encode_together = ['player_positions', 'club_position', 'nation_position', 'preferred_foot', 'work_rate',
                              'body_type', 'player_tags', 'player_traits']

# Applying one-hot encoding to the selected columns
df_encoded = pd.get_dummies(df, columns=columns_to_encode_together)

# Displaying the encoded DataFrame
pd.set_option('display.max_columns', 1764)
df_encoded

"""SELECTING DEPENDENT AND INDEPENDENT VARIABLES"""

y = df_encoded['overall']
x = df_encoded.drop(['overall'], axis=1)

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

"""SCALING THE DATA"""

scaler = StandardScaler()
numeric_features = x.select_dtypes(include=['int', 'float']).columns
x_train_scaled = scaler.fit_transform(x_train[numeric_features])
x_test_scaled = scaler.transform(x_test[numeric_features])

print(x_train_scaled)
print(x_test_scaled)

"""CORRELATION MATRIX- to provide a data-based justification of  why certain columns were dropped"""

corr_matrix = df.corr()
plt.figure(figsize=(12, 8))
plt.imshow(corr_matrix, cmap='coolwarm', interpolation='nearest')
plt.colorbar()
plt.title('Correlation Matrix')
plt.xticks(range(len(corr_matrix.columns)), corr_matrix.columns, rotation=90)
plt.yticks(range(len(corr_matrix.columns)), corr_matrix.columns)
plt.show()

'''
from sklearn.feature_selection import SelectKBest, f_regression
from sklearn.model_selection import train_test_split

# Load your dataset (replace 'data.csv' with your dataset)
#data = pd.read_csv('data.csv')

# Define your features and target variable
#X = data.drop(columns=['player_rating'])
#y = data['player_rating']

# Split the data into training and testing sets
#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Select the top 'k' best features
k = 5  # Set the desired number of features
selector = SelectKBest(score_func=f_regression, k=k)
X_train_new = selector.fit_transform(X_train, y_train)
X_test_new = selector.transform(X_test)
'''

"""TRAINING WITH CROSS VALIDATION"""

# RANDOM FOREST

rf = RandomForestRegressor()
rf.fit(x_train_scaled, y_train)

# Cross-validation for Random Forest
rf_scores = cross_val_score(rf, x_train_scaled, y_train, cv=5, scoring='neg_mean_squared_error')
rf_rmse_scores = np.sqrt(-rf_scores)

# Optimizing Random Forest model performan
param_grid = [
    {'n_estimators': [1400, 1500, 2000], 'max_features': [40, 50, 60]}
    # {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},
]

grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='neg_mean_squared_error')
grid_search.fit(x_train_scaled, y_train)
rf_best_params = grid_search.best_params_

# Random Forest Evaluation
rf_pred = rf.predict(x_test_scaled)
rf_mse = mean_squared_error(y_test, rf_pred)
rf_rmse = np.sqrt(rf_mse)

print("Random Forest RMSE during Cross-Validation:", rf_rmse_scores)
print("Best Parameters for Random Forest:", rf_best_params)
print("Random Forest RMSE on Validation Set:", rf_rmse)

# XGBOOST

xg_reg = XGBRegressor(objective='reg:squarederror')
xg_reg.fit(x_train_scaled, y_train)

# Cross-validation for XGBoost
xgb_scores = cross_val_score(xg_reg, x_train_scaled, y_train, cv=5, scoring='neg_mean_squared_error')
xgb_rmse_scores = np.sqrt(-xgb_scores)

# selecting best parameters to optimize XGBoost performance
params = {"learning_rate": [0.01, 0.1, 0.2], "max_depth": [2000], "min_child_weight": [5, 10, 15]}
xgb_grid = GridSearchCV(xg_reg, params, cv=5, scoring='neg_mean_squared_error')
xgb_grid.fit(x_train_scaled, y_train)
xgb_best_params = xgb_grid.best_params_

# XGBoost evaluation
xgb_pred = xg_reg.predict(x_test_scaled)
xgb_mse = mean_squared_error(y_test, xgb_pred)
xgb_rmse = np.sqrt(xgb_mse)

print("\nXGBoost RMSE during Cross-Validation:", xgb_rmse_scores)
print("Best Parameters for XGBoost:", xgb_best_params)
print("XGBoost RMSE on Validation Set:", xgb_rmse)

# GRADIENT BOOSTING REGRESSOR

gbr = GradientBoostingRegressor()
gbr.fit(x_train_scaled, y_train)

# Cross-validation for Gradient Boosting Regressor
gbr_scores = cross_val_score(gbr, x_train_scaled, y_train, cv=5, scoring='neg_mean_squared_error')
gbr_rmse_scores = np.sqrt(-gbr_scores)

# Hyperparameter tuning for Gradient Boosting Regressor
param_grid_gbr = {'n_estimators': [100, 500, 1000, 2000], 'learning_rate': [0.01, 0.1, 0.2, 0.5, 1]}
gbr_grid = GridSearchCV(gbr, param_grid_gbr, cv=5, scoring='neg_mean_squared_error')
gbr_grid.fit(x_train_scaled, y_train)
gbr_best_params = gbr_grid.best_params_

# Gradient Boosting evaluation
gbr_pred = gbr.predict(x_test_scaled)
gbr_mse = mean_squared_error(y_test, gbr_pred)
gbr_rmse = np.sqrt(gbr_mse)

print("\nGradient Boosting Regressor RMSE during Cross-Validation:", gbr_rmse_scores)
print("Best Parameters for Gradient Boosting Regressor:", gbr_best_params)
print("Gradient Boosting Regressor RMSE on Validation Set:", gbr_rmse)

"""DISPLAYING MODEL PERFORMANCE AND EVALUATION"""

print("Random Forest Validation Score:", rf_pred)
print("XGBoost Validation Score:", xgb_pred)
print("Gradient Boosting Validation Score:", gbr_pred)
print()

print("Random Forest root mean square error:", rf_rmse)
print("XGBoost root mean square error:", xgb_rmse)
print("Gradient Boosting root mean square error:", gbr_rmse)

"""XGBOOST"""

# Evaluate the final model
'''
final_model = XGBRegressor()  # Replace with the best-performing model
final_model.fit(x_train, y_train)
y_pred = final_model.predict(x_test)
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print(f"Final Model MAE: {mae}")
print(f"Final Model RMSE: {rmse}")'''

"""TESTING THE MODEL USING THE 2022 DATASET"""

# LOADING AND CLEANING THE DATA

df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/players_22.csv')

df = df.drop(
    ['sofifa_id', 'player_url', 'player_face_url', 'real_face', 'club_logo_url', 'club_flag_url', 'nation_logo_url',
     'nation_flag_url', 'dob', 'club_contract_valid_until',
     'club_joined', 'club_jersey_number', 'nationality_id', 'club_loaned_from', 'nation_jersey_number', 'real_face',
     'club_team_id', 'nation_team_id',
     ], axis=1)

numeric_columns = df.select_dtypes(include=[np.number]).columns
non_numeric_columns = df.select_dtypes(exclude=[np.number]).columns

numeric_imputer = SimpleImputer(strategy='mean')
non_numeric_imputer = SimpleImputer(strategy='most_frequent')

df[numeric_columns] = numeric_imputer.fit_transform(df[numeric_columns])
df[non_numeric_columns] = non_numeric_imputer.fit_transform(df[non_numeric_columns])

columns_to_encode_together = ['player_positions', 'club_position', 'nation_position', 'preferred_foot', 'work_rate',
                              'body_type', 'player_tags', 'player_traits']

df_encoded = pd.get_dummies(df, columns=columns_to_encode_together)

y = df_encoded['overall']
x = df_encoded.drop(['overall'], axis=1)

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
numeric_features = x.select_dtypes(include=['int', 'float']).columns
x_player22_train_scaled = scaler.fit_transform(x_train[numeric_features])
x_player22_test_scaled = scaler.transform(x_test[numeric_features])

# TESTING THE GRADIENT BOOSTING REGRESSOR MODEL
gbr.predict(x_player22_test_scaled)



"""MAKING CONNECTION WITH API"""

from flask import Flask, render_template, request
import joblib

app = Flask(__name__)

joblib.dump(gbr, 'Group13_SportsPrediction.joblib')
model = joblib.load('Group13_SportsPrediction.joblib')

# pickle.dump(gbr, open('/content/drive/My Drive/Colab Notebooks/Group13_SportsPrediction.pkl', 'wb'))

# model = pickle.load(open('/content/drive/My Drive/Colab Notebooks/Group13_SportsPrediction.pkl', 'rb'))
